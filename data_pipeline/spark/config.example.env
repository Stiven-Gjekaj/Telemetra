# Telemetra Spark Streaming Job Configuration
# Copy this file to .env and adjust values as needed

# ============================================
# Kafka Configuration
# ============================================
KAFKA_BOOTSTRAP_SERVERS=kafka:9092

# For multiple brokers (production):
# KAFKA_BOOTSTRAP_SERVERS=kafka-1:9092,kafka-2:9092,kafka-3:9092

# ============================================
# PostgreSQL Configuration
# ============================================
POSTGRES_URL=jdbc:postgresql://postgres:5432/telemetra
POSTGRES_USER=telemetra
POSTGRES_PASSWORD=telemetra

# For production with SSL:
# POSTGRES_URL=jdbc:postgresql://postgres:5432/telemetra?ssl=true&sslmode=require

# ============================================
# Spark Configuration
# ============================================

# Spark Master URL
# Options:
#   local[*]               - Run locally with all cores
#   local[4]               - Run locally with 4 cores
#   spark://master:7077    - Connect to Spark standalone cluster
#   yarn                   - YARN cluster mode
#   k8s://https://...      - Kubernetes cluster
SPARK_MASTER=local[*]

# Memory Configuration
SPARK_DRIVER_MEMORY=2g
SPARK_EXECUTOR_MEMORY=2g
SPARK_EXECUTOR_CORES=2

# For production, adjust based on available resources:
# SPARK_DRIVER_MEMORY=4g
# SPARK_EXECUTOR_MEMORY=8g
# SPARK_EXECUTOR_CORES=4

# ============================================
# Streaming Job Configuration
# ============================================

# Checkpoint directory for fault tolerance
# Use distributed storage in production (HDFS/S3):
CHECKPOINT_LOCATION=/tmp/spark-checkpoints
# CHECKPOINT_LOCATION=hdfs://namenode:9000/checkpoints/telemetra
# CHECKPOINT_LOCATION=s3a://telemetra-checkpoints/spark/

# Anomaly detection threshold (z-score)
# Higher = fewer anomalies detected
# Recommended: 2.5 - 4.0
ANOMALY_THRESHOLD=3.0

# ============================================
# Performance Tuning
# ============================================

# Maximum records to process per trigger (rate limiting)
MAX_OFFSETS_PER_TRIGGER=10000

# Micro-batch interval
TRIGGER_INTERVAL=10 seconds

# Watermark delay for late data
WATERMARK_DELAY=10 seconds

# Window configuration
WINDOW_DURATION=1 minute
SLIDE_DURATION=10 seconds

# ============================================
# Monitoring & Logging
# ============================================

# Python logging level
LOG_LEVEL=INFO

# Spark UI port
SPARK_UI_PORT=4040

# Enable Spark event logging
SPARK_EVENTLOG_ENABLED=true
SPARK_EVENTLOG_DIR=/tmp/spark-events

# ============================================
# Development Settings
# ============================================

# Set to 'true' for local development
DEV_MODE=false

# Override Kafka offset start position in dev
# Options: earliest, latest
KAFKA_STARTING_OFFSETS=latest

# ============================================
# Security (Production)
# ============================================

# Kafka SSL/SASL Configuration
# KAFKA_SECURITY_PROTOCOL=SASL_SSL
# KAFKA_SASL_MECHANISM=PLAIN
# KAFKA_SASL_JAAS_CONFIG=...

# PostgreSQL SSL
# POSTGRES_SSL_MODE=require
# POSTGRES_SSL_CERT=/path/to/client-cert.pem
# POSTGRES_SSL_KEY=/path/to/client-key.pem
# POSTGRES_SSL_ROOT_CERT=/path/to/ca-cert.pem

# ============================================
# Resource Limits (Docker)
# ============================================

# CPU and memory limits
DOCKER_CPU_LIMIT=2.0
DOCKER_MEMORY_LIMIT=4G
DOCKER_CPU_RESERVATION=1.0
DOCKER_MEMORY_RESERVATION=2G
